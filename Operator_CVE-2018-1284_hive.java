public class Operator {
  private void baseForward(Object row, ObjectInspector rowInspector)
      throws HiveException {
    this.runTimeNumRows++;
    if (getDone()) {
      return;
    }

    int childrenDone = 0;
    for (int i = 0; i < childOperatorsArray.length; i++) {
      Operator<? extends OperatorDesc> o = childOperatorsArray[i];
      if (o.getDone()) {
        childrenDone++;
      } else {
        o.process(row, childOperatorsTag[i]);
      }
    }

    // if all children are done, this operator is also done
    if (childrenDone != 0 && childrenDone == childOperatorsArray.length) {
      setDone(true);
    }
  }

  protected final void setDone(boolean done) {
    this.done = done;
  }

  @Override
  public Operator<? extends OperatorDesc> clone()
    throws CloneNotSupportedException {

    List<Operator<? extends OperatorDesc>> parents = getParentOperators();
    List<Operator<? extends OperatorDesc>> parentClones =
      new ArrayList<Operator<? extends OperatorDesc>>();

    if (parents != null) {
      for (Operator<? extends OperatorDesc> parent : parents) {
        parentClones.add((parent.clone()));
      }
    }

    @SuppressWarnings("unchecked")
    T descClone = (T)conf.clone();
    // also clone the colExprMap by default
    // we need a deep copy
    ArrayList<ColumnInfo> colInfos = new ArrayList<>();
    colInfos.addAll(getSchema().getSignature());
    Map<String, ExprNodeDesc> map = null;
    if (getColumnExprMap() != null) {
      map = new HashMap<>();
      map.putAll(getColumnExprMap());
    }
    Operator<? extends OperatorDesc> ret = OperatorFactory.getAndMakeChild(
            cContext, descClone, new RowSchema(colInfos), map, parentClones);

    return ret;
  }

  public String getReduceOutputName() {
    return null;
  }

  public Object getGroupKeyObject() {
    return groupKeyObject;
  }

  public void logStats() {
    if (LOG.isInfoEnabled() && !statsMap.isEmpty()) {
      StringBuilder sb = new StringBuilder();
      for (Map.Entry<String, LongWritable> e : statsMap.entrySet()) {
        sb.append(e.getKey()).append(":").append(e.getValue()).append(", ");
      }
      LOG.info(sb.toString());
    }
  }

  public Map<String, ExprNodeDesc> getColumnExprMap() {
    if(this.getConf() == null) {
      return null;
    }
    return this.getConf().getColumnExprMap();
  }

    @Override
    public OperatorType getType() {
      return null;
    }

  public boolean supportUnionRemoveOptimization() {
    return false;
  }



  @Override
  public ArrayList<Node> getChildren() {

    if (getChildOperators() == null) {
      return null;
    }

    ArrayList<Node> ret_vec = new ArrayList<Node>();
    for (Operator<? extends OperatorDesc> op : getChildOperators()) {
      ret_vec.add(op);
    }

    return ret_vec;
  }


  public void setId(String id) {
    this.id = id;
  }

  public Statistics getStatistics() {
    if (conf != null) {
      return conf.getStatistics();
    }

    return null;
  }

  @SuppressWarnings("rawtypes")
  public static Operator createDummy() {
    return new DummyOperator();
  }

  @SuppressWarnings({ "serial", "unchecked", "rawtypes" }

  protected List<String> getAdditionalCounters() {
    return null;
  }

  private void publishRunTimeStats() throws HiveException {
    StatsPublisher statsPublisher = new FSStatsPublisher();
    StatsCollectionContext sContext = new StatsCollectionContext(hconf);
    sContext.setIndexForTezUnion(indexForTezUnion);
    sContext.setStatsTmpDir(conf.getRuntimeStatsTmpDir());

    if (!statsPublisher.connect(sContext)) {
      LOG.error("StatsPublishing error: cannot connect to database");
      throw new HiveException(ErrorMsg.STATSPUBLISHER_CONNECTION_ERROR.getErrorCodedMsg());
    }

    String prefix = "";
    Map<String, String> statsToPublish = new HashMap<String, String>();
    statsToPublish.put(StatsSetupConst.RUN_TIME_ROW_COUNT, Long.toString(runTimeNumRows));
    if (!statsPublisher.publishStat(prefix, statsToPublish)) {
      // The original exception is lost.
      // Not changing the interface to maintain backward compatibility
      throw new HiveException(ErrorMsg.STATSPUBLISHER_PUBLISHING_ERROR.getErrorCodedMsg());
    }
    if (!statsPublisher.closeConnection(sContext)) {
      // The original exception is lost.
      // Not changing the interface to maintain backward compatibility
      throw new HiveException(ErrorMsg.STATSPUBLISHER_CLOSING_ERROR.getErrorCodedMsg());
    }
  }

  public void passExecContext(ExecMapperContext execContext) {
    this.setExecContext(execContext);
    for (int i = 0; i < childOperators.size(); i++) {
        childOperators.get(i).passExecContext(execContext);
    }
  }

  public Operator<? extends OperatorDesc> cloneRecursiveChildren()
      throws CloneNotSupportedException {
    Operator<? extends OperatorDesc> newOp = this.cloneOp();
    newOp.setParentOperators(this.parentOperators);
    List<Operator<? extends OperatorDesc>> newChildren =
        new ArrayList<Operator<? extends OperatorDesc>>();

    for (Operator<? extends OperatorDesc> childOp : this.getChildOperators()) {
      List<Operator<? extends OperatorDesc>> parentList =
          new ArrayList<Operator<? extends OperatorDesc>>();
      for (Operator<? extends OperatorDesc> parent : childOp.getParentOperators()) {
        if (parent.equals(this)) {
          parentList.add(newOp);
        } else {
          parentList.add(parent);
        }
      }
      // Recursively clone the children
      Operator<? extends OperatorDesc> clonedChildOp = childOp.cloneRecursiveChildren();
      clonedChildOp.setParentOperators(parentList);
    }

    newOp.setChildOperators(newChildren);
    return newOp;
  }

  public final boolean logicalEqualsTree(Operator<?> o) {
    // XXX: this could easily become a hot-spot
    if (!logicalEquals(o)) {
      return false;
    }
    if (o.getNumParent() != getNumParent()) {
      return false;
    }
    for (int i = 0; i < getNumParent(); i++) {
      Operator<? extends OperatorDesc> copL = parentOperators.get(i);
      Operator<? extends OperatorDesc> copR = o.parentOperators.get(i);
      if (!copL.logicalEquals(copR)) {
        return false;
      }
    }
    return true;
  }

  private void vectorForward(VectorizedRowBatch vrg, ObjectInspector rowInspector)
      throws HiveException {
    this.runTimeNumRows += vrg.count();
    if (getDone()) {
      return;
    }

    // Data structures to store original values
    final int size = vrg.size;
    final boolean selectedInUse = vrg.selectedInUse;
    final boolean saveState = (selectedInUse && multiChildren);
    if (saveState) {
      System.arraycopy(vrg.selected, 0, selected, 0, size);
    }

    int childrenDone = 0;
    for (int i = 0; i < childOperatorsArray.length; i++) {
      Operator<? extends OperatorDesc> o = childOperatorsArray[i];
      if (o.getDone()) {
        childrenDone++;
      } else {
        o.process(vrg, childOperatorsTag[i]);
        // Restore original values
        vrg.size = size;
        vrg.selectedInUse = selectedInUse;
        if (saveState) {
          System.arraycopy(selected, 0, vrg.selected, 0, size);
        }
      }
    }

    // if all children are done, this operator is also done
    if (childrenDone != 0 && childrenDone == childOperatorsArray.length) {
      setDone(true);
    }
  }

  public Map<String, Long> getStats() {
    HashMap<String, Long> ret = new HashMap<String, Long>();
    for (String one : statsMap.keySet()) {
      ret.put(one, Long.valueOf(statsMap.get(one).get()));
    }
    return (ret);
  }

  public void abort() {
    LOG.info("Received abort in operator: {}", getName());
    abortOp.set(true);
  }

  public void setAlias(String alias) {
    this.alias = alias;

    for (Operator<? extends OperatorDesc> op : childOperators) {
      op.setAlias(alias);
    }
  }

    @Override
    public String getName() {
      return DummyOperator.getOperatorName();
    }

  public String getCounterName(Counter counter, Configuration hconf) {
    String context = hconf.get(Operator.CONTEXT_NAME_KEY, "");
    if (context != null && !context.isEmpty()) {
      context = "_" + context.replace(" ", "_");
    }
    return counter + context;
  }

  public void setGroupKeyObject(Object keyObject) {
    this.groupKeyObject = keyObject;
  }

  protected static StructObjectInspector initEvaluatorsAndReturnStruct(
      ExprNodeEvaluator<?>[] evals, List<String> outputColName,
      ObjectInspector rowInspector) throws HiveException {
    ObjectInspector[] fieldObjectInspectors = initEvaluators(evals,
        rowInspector);
    return ObjectInspectorFactory.getStandardStructObjectInspector(
        outputColName, Arrays.asList(fieldObjectInspectors));
  }

  public void setInputContext(String tableName, String partitionName) {
    if (childOperators != null) {
      for (Operator<? extends OperatorDesc> child : childOperators) {
        if (child.getNumParent() == 1) {
          child.setInputContext(tableName, partitionName);
        }
      }
    }
  }

  public boolean columnNamesRowResolvedCanBeObtained() {
    return false;
  }

  protected boolean allInitializedParentsAreClosed() {
    if (parentOperators != null) {
      for (Operator<? extends OperatorDesc> parent : parentOperators) {
        if(parent==null){
          continue;
        }
        if (LOG.isDebugEnabled()) {
          LOG.debug("allInitializedParentsAreClosed? parent.state = " + parent.state);
        }
        if (!(parent.state == State.CLOSE || parent.state == State.UNINIT)) {
          return false;
        }
      }
    }
    return true;
  }

  public String getIdentifier() {
    return id;
  }

  public void close(boolean abort) throws HiveException {
    if (LOG.isDebugEnabled()) {
      LOG.debug("close called for operator " + this);
    }

    if (state == State.CLOSE) {
      return;
    }

    // check if all parents are finished
    if (!allInitializedParentsAreClosed()) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("Not all parent operators are closed. Not closing.");
      }
      return;
    }

    // set state as CLOSE as long as all parents are closed
    // state == CLOSE doesn't mean all children are also in state CLOSE
    state = State.CLOSE;
    if (LOG.isDebugEnabled()) {
      LOG.info("Closing operator " + this);
    }

    abort |= abortOp.get();


    // call the operator specific close routine
    closeOp(abort);

    // closeOp can be overriden
    if (conf != null && conf.getRuntimeStatsTmpDir() != null) {
      publishRunTimeStats();
    }
    LongWritable runTimeRowsWritable = new LongWritable(runTimeNumRows);
    LongWritable recordCounter = new LongWritable(numRows);
    statsMap.put(Counter.RECORDS_OUT_OPERATOR.name() + "_" + getOperatorId(), runTimeRowsWritable);
    statsMap.put(getCounterName(Counter.RECORDS_OUT_INTERMEDIATE, hconf), recordCounter);
    this.runTimeNumRows = 0;

    reporter = null;

    try {
      logStats();
      if (childOperators == null) {
        return;
      }

      for (Operator<? extends OperatorDesc> op : childOperators) {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Closing child = " + op);
        }
        op.close(abort);
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug(id + " Close done");
      }
    } catch (HiveException e) {
      LOG.warn("Caught exception while closing operator: " + e.getMessage(), e);
      throw e;
    }
  }

  private String getLevelString(int level) {
    if (level == 0) {
      return "\n";
    }
    StringBuilder s = new StringBuilder();
    s.append("\n");
    while (level > 0) {
      s.append("  ");
      level--;
    }
    return s.toString();
  }

  static boolean toString(StringBuilder builder, Set<String> visited, Operator<?> op, int start) {
    String name = op.toString();
    boolean added = visited.add(name);
    if (start > 0) {
      builder.append("-");
      start++;
    }
    builder.append(name);
    start += name.length();
    if (added) {
      if (op.getNumChild() > 0) {
        List<Operator<?>> children = op.getChildOperators();
        for (int i = 0; i < children.size(); i++) {
          if (i > 0) {
            builder.append('\n');
            for (int j = 0; j < start; j++) {
              builder.append(' ');
            }
          }
          toString(builder, visited, children.get(i), start);
        }
      }
      return true;
    }
    return false;
  }

  public String dump(int level, HashSet<Integer> seenOpts) {
    if (seenOpts.contains(new Integer(id))) {
      return null;
    }
    seenOpts.add(new Integer(id));

    StringBuilder s = new StringBuilder();
    String ls = getLevelString(level);
    s.append(ls);
    s.append("<" + getName() + ">");
    s.append("Id =" + id);

    if (childOperators != null) {
      s.append(ls);
      s.append("  <Children>");
      for (Operator<? extends OperatorDesc> o : childOperators) {
        s.append(o.dump(level + 2, seenOpts));
      }
      s.append(ls);
      s.append("  <\\Children>");
    }

    if (parentOperators != null) {
      s.append(ls);
      s.append("  <Parent>");
      for (Operator<? extends OperatorDesc> o : parentOperators) {
        s.append("Id = " + o.id + " ");
        s.append(o.dump(level, seenOpts));
      }
      s.append("<\\Parent>");
    }

    s.append(ls);
    s.append("<\\" + getName() + ">");
    return s.toString();
  }

  protected void initialize(Configuration hconf, ObjectInspector inputOI,
      int parentId) throws HiveException {
    if (LOG.isDebugEnabled()) {
      LOG.debug("Initializing child " + id + " " + getName());
    }
    // Double the size of the array if needed
    if (parentId >= inputObjInspectors.length) {
      int newLength = inputObjInspectors.length * 2;
      while (parentId >= newLength) {
        newLength *= 2;
      }
      inputObjInspectors = Arrays.copyOf(inputObjInspectors, newLength);
    }
    inputObjInspectors[parentId] = inputOI;
    // call the actual operator initialization function
    initialize(hconf, null);
  }

  public boolean acceptLimitPushdown() {
    return false;
  }

    @Override
    protected void initializeOp(Configuration conf) {
    }
  }

  public void removeParents() {
    for (Operator<?> parent : new ArrayList<Operator<?>>(getParentOperators())) {
      removeParent(parent);
    }
  }

  public boolean getIsReduceSink() {
    return false;
  }

  public String getReduceOutputName() {
    return null;
  }

  public void setCompilationOpContext(CompilationOpContext ctx) {
    cContext = ctx;
  }

  public CompilationOpContext getCompilationOpContext() {
    return cContext;
  }

  private void publishRunTimeStats() throws HiveException {
    StatsPublisher statsPublisher = new FSStatsPublisher();
    StatsCollectionContext sContext = new StatsCollectionContext(hconf);
    sContext.setIndexForTezUnion(indexForTezUnion);
    sContext.setStatsTmpDir(conf.getRuntimeStatsTmpDir());

    if (!statsPublisher.connect(sContext)) {
      LOG.error("StatsPublishing error: cannot connect to database");
      throw new HiveException(ErrorMsg.STATSPUBLISHER_CONNECTION_ERROR.getErrorCodedMsg());
    }

    String prefix = "";
    Map<String, String> statsToPublish = new HashMap<String, String>();
    statsToPublish.put(StatsSetupConst.RUN_TIME_ROW_COUNT, Long.toString(runTimeNumRows));
    if (!statsPublisher.publishStat(prefix, statsToPublish)) {
      // The original exception is lost.
      // Not changing the interface to maintain backward compatibility
      throw new HiveException(ErrorMsg.STATSPUBLISHER_PUBLISHING_ERROR.getErrorCodedMsg());
    }
    if (!statsPublisher.closeConnection(sContext)) {
      // The original exception is lost.
      // Not changing the interface to maintain backward compatibility
      throw new HiveException(ErrorMsg.STATSPUBLISHER_CLOSING_ERROR.getErrorCodedMsg());
    }
  }

  public int getIndexForTezUnion() {
    return indexForTezUnion;
  }

  public void setIndexForTezUnion(int indexForTezUnion) {
    this.indexForTezUnion = indexForTezUnion;
  }

  public boolean logicalEquals(Operator other) {
    return getClass().getName().equals(other.getClass().getName()) &&
        (conf == other.getConf() || (conf != null && other.getConf() != null &&
            conf.isSame(other.getConf())));
  }

  // Currently only used during re-optimization related parts.
  // FIXME: HIVE-18703 should probably move this method somewhere else
  public final boolean logicalEqualsTree(Operator<?> o) {
    // XXX: this could easily become a hot-spot
    if (!logicalEquals(o)) {
      return false;
    }
    if (o.getNumParent() != getNumParent()) {
      return false;
    }
    for (int i = 0; i < getNumParent(); i++) {
      Operator<? extends OperatorDesc> copL = parentOperators.get(i);
      Operator<? extends OperatorDesc> copR = o.parentOperators.get(i);
      if (!copL.logicalEquals(copR)) {
        return false;
      }
    }

  private void cancelAsyncInitOps() {
    for (Future<?> f : asyncInitOperations) {
      f.cancel(true);
    }
    asyncInitOperations.clear();
  }

  public boolean getIsReduceSink() {
    return false;
  }

  protected boolean areAllParentsInitialized() {
    for (Operator<? extends OperatorDesc> parent : parentOperators) {
      if (parent == null) {
        //return true;
        continue;
      }
      if (parent.state != State.INIT) {
        return false;
      }
    }
    return true;
  }

  public String getOperatorId() {
    return operatorId;
  }

  @SuppressWarnings("unchecked")
  public Operator<? extends OperatorDesc> cloneOp() throws CloneNotSupportedException {
    T descClone = (T) conf.clone();
    Operator<? extends OperatorDesc> ret =
        OperatorFactory.getAndMakeChild(cContext, descClone, getSchema());
    return ret;
  }

  public boolean supportAutomaticSortMergeJoin() {
    return false;
  }

  public boolean logicalEquals(Operator other) {
    return getClass().getName().equals(other.getClass().getName()) &&
        (conf == other.getConf() || (conf != null && other.getConf() != null &&
            conf.isSame(other.getConf())));
  }

  public void replaceChild(Operator<? extends OperatorDesc> child,
      Operator<? extends OperatorDesc> newChild) {
    int childIndex = childOperators.indexOf(child);
    assert childIndex != -1;
    childOperators.set(childIndex, newChild);
  }

    public static String getOperatorName() {
      return "DUMMY";
    }

  public boolean supportSkewJoinOptimization() {
    return false;
  }

  public void replaceParent(Operator<? extends OperatorDesc> parent,
      Operator<? extends OperatorDesc> newParent) {
    int parentIndex = parentOperators.indexOf(parent);
    assert parentIndex != -1;
    parentOperators.set(parentIndex, newParent);
  }

  private void completeInitialization(Collection<Future<?>> fs) throws HiveException {
    Object[] os = new Object[fs.size()];
    int i = 0;
    Throwable asyncEx = null;

    // Wait for all futures to complete. Check for an abort while waiting for each future. If any of the futures is cancelled / aborted - cancel all subsequent futures.

    boolean cancelAll = false;
    for (Future<?> f : fs) {
      // If aborted - break out of the loop, and cancel all subsequent futures.
      if (cancelAll) {
        break;
      }
      if (abortOp.get()) {
        cancelAll = true;
        break;
      } else {
        // Wait for the current future.
        while (true) {
          if (abortOp.get()) {
            cancelAll = true;
            break;
          } else {
            try {
              // Await future result with a timeout to check the abort field occasionally.
              // It's possible that the interrupt which comes in along with an abort, is suppressed
              // by some other operator.
              Object futureResult = f.get(200l, TimeUnit.MILLISECONDS);
              os[i++] = futureResult;
              break;
            } catch (TimeoutException e) {
              // Expected if the operation takes time. Continue the loop, and wait for op completion.
            } catch (InterruptedException | CancellationException e) {
              asyncEx = e;
              cancelAll = true;
              break;
            } catch (ExecutionException e) {
              if (e.getCause() == null) {
                asyncEx = e;
              } else {
                asyncEx = e.getCause();
              }
              cancelAll = true;
              break;
            }
          }
        }

      }
    }

    if (cancelAll || asyncEx != null) {
      for (Future<?> f : fs) {
        // It's ok to send a cancel to an already completed future. Is a no-op
        f.cancel(true);
      }
      throw new HiveException("Async Initialization failed. abortRequested=" + abortOp.get(), asyncEx);
    }


    completeInitializationOp(os);
  }

  public void removeParents() {
    for (Operator<?> parent : new ArrayList<Operator<?>>(getParentOperators())) {
      removeParent(parent);
    }
  }

  public void setColumnExprMap(Map<String, ExprNodeDesc> colExprMap) {
    if(this.getConf() != null) {
      this.getConf().setColumnExprMap(colExprMap);
    }
  }

}