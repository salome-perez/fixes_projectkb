public class GridCachePartitionExchangeManager {
    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {
        if (!enterBusy())
            return;

        try {
            if (msg.exchangeId() == null) {
                if (log.isDebugEnabled())
                    log.debug("Received local partition update [nodeId=" + node.id() + ", parts=" +
                        msg + ']');

                boolean updated = false;

                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {
                    Integer grpId = entry.getKey();

                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);

                    if (grp != null &&
                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)
                        continue;

                    GridDhtPartitionTopology top = null;

                    if (grp == null)
                        top = clientTops.get(grpId);
                    else if (!grp.isLocal())
                        top = grp.topology();

                    if (top != null) {
                        updated |= top.update(null, entry.getValue(), false);

                        cctx.affinity().checkRebalanceState(top, grpId);
                    }
                }

                if (updated)
                    scheduleResendPartitions();
            }
            else {
                GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId(), null, null, null, null);

                if (log.isDebugEnabled())
                    log.debug("Notifying exchange future about single message: " + exchFut);

                if (msg.client() && !exchFut.isDone()) {
                    if (exchFut.initialVersion().compareTo(readyAffinityVersion()) <= 0) {
                        U.warn(log, "Client node tries to connect but its exchange " +
                            "info is cleaned up from exchange history." +
                            " Consider increasing 'IGNITE_EXCHANGE_HISTORY_SIZE' property " +
                            "or start clients in  smaller batches."
                        );

                        exchFut.forceClientReconnect(node, msg);

                        return;
                    }
                }

                exchFut.onReceiveSingleMessage(node, msg);
            }
        }
        finally {
            leaveBusy();
        }
    }

    private void body0() throws InterruptedException, IgniteCheckedException {
        long timeout = cctx.gridConfig().getNetworkTimeout();

        long cnt = 0;

        while (!isCancelled()) {
            cnt++;

            CachePartitionExchangeWorkerTask task = null;

            try {
                boolean preloadFinished = true;

                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {
                    if (grp.isLocal())
                        continue;

                    preloadFinished &= grp.preloader() != null && grp.preloader().syncFuture().isDone();

                    if (!preloadFinished)
                        break;
                }

                // If not first preloading and no more topology events present.
                if (!cctx.kernalContext().clientNode() && !hasPendingExchange() && preloadFinished)
                    timeout = cctx.gridConfig().getNetworkTimeout();

                // After workers line up and before preloading starts we initialize all futures.
                if (log.isDebugEnabled()) {
                    Collection<IgniteInternalFuture> unfinished = new HashSet<>();

                    for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {
                        if (!fut.isDone())
                            unfinished.add(fut);
                    }

                    log.debug("Before waiting for exchange futures [futs" + unfinished + ", worker=" + this + ']');
                }

                // Take next exchange future.
                if (isCancelled())
                    Thread.currentThread().interrupt();

                task = futQ.poll(timeout, MILLISECONDS);

                if (task == null)
                    continue; // Main while loop.

                if (!isExchangeTask(task)) {
                    processCustomTask(task);

                    continue;
                }

                busy = true;

                Map<Integer, GridDhtPreloaderAssignments> assignsMap = null;

                boolean forcePreload = false;

                GridDhtPartitionExchangeId exchId;

                GridDhtPartitionsExchangeFuture exchFut = null;

                AffinityTopologyVersion resVer = null;

                try {
                    if (isCancelled())
                        break;

                    if (task instanceof RebalanceReassignExchangeTask) {
                        exchId = ((RebalanceReassignExchangeTask) task).exchangeId();
                    }
                    else if (task instanceof ForceRebalanceExchangeTask) {
                        forcePreload = true;

                        timeout = 0; // Force refresh.

                        exchId = ((ForceRebalanceExchangeTask)task).exchangeId();
                    }
                    else {
                        assert task instanceof GridDhtPartitionsExchangeFuture : task;

                        exchFut = (GridDhtPartitionsExchangeFuture)task;

                        exchId = exchFut.exchangeId();

                        lastInitializedFut = exchFut;

                        boolean newCrd = false;

                        if (!crd) {
                            List<ClusterNode> srvNodes = exchFut.firstEventCache().serverNodes();

                            crd = newCrd = !srvNodes.isEmpty() && srvNodes.get(0).isLocal();
                        }

                        exchFut.init(newCrd);

                        int dumpCnt = 0;

                        IgniteConfiguration cfg = cctx.gridConfig();

                        long rollbackTimeout = cfg.getTransactionConfiguration().getTxTimeoutOnPartitionMapExchange();

                        final long dumpTimeout = 2 * cctx.gridConfig().getNetworkTimeout();

                        long nextDumpTime = 0;

                        while (true) {
                            try {
                                resVer = exchFut.get(rollbackTimeout > 0 ? rollbackTimeout : dumpTimeout);

                                break;
                            }
                            catch (IgniteFutureTimeoutCheckedException ignored) {
                                if (nextDumpTime <= U.currentTimeMillis()) {
                                    U.warn(diagnosticLog, "Failed to wait for partition map exchange [" +
                                        "topVer=" + exchFut.initialVersion() +
                                        ", node=" + cctx.localNodeId() + "]. " +
                                        (rollbackTimeout == 0 ? "Consider changing TransactionConfiguration.txTimeoutOnPartitionMapSynchronization to non default value to avoid this message. " : "") +
                                        "Dumping pending objects that might be the cause: ");

                                    try {
                                        dumpDebugInfo(exchFut);
                                    }
                                    catch (Exception e) {
                                        U.error(diagnosticLog, "Failed to dump debug information: " + e, e);
                                    }

                                    nextDumpTime = U.currentTimeMillis() + nextDumpTimeout(dumpCnt++, dumpTimeout);
                                }

                                if (rollbackTimeout > 0) {
                                    rollbackTimeout = 0; // Try automatic rollback only once.

                                    cctx.tm().rollbackOnTopologyChange(exchFut.initialVersion());
                                }
                            }
                            catch (Exception e) {
                                if (exchFut.reconnectOnError(e))
                                    throw new IgniteNeedReconnectException(cctx.localNode(), e);

                                throw e;
                            }
                        }

                        removeMergedFutures(resVer, exchFut);

                        if (log.isDebugEnabled())
                            log.debug("After waiting for exchange future [exchFut=" + exchFut + ", worker=" +
                                this + ']');

                        if (exchFut.exchangeId().nodeId().equals(cctx.localNodeId()))
                            lastRefresh.compareAndSet(-1, U.currentTimeMillis());

                        // Just pick first worker to do this, so we don't
                        // invoke topology callback more than once for the
                        // same event.

                        boolean changed = false;

                        for (CacheGroupContext grp : cctx.cache().cacheGroups()) {
                            if (grp.isLocal())
                                continue;

                            changed |= grp.topology().afterExchange(exchFut);
                        }

                        if (!cctx.kernalContext().clientNode() && changed && !hasPendingServerExchange())
                            refreshPartitions();
                    }

                    if (!cctx.kernalContext().clientNode()) {
                        assignsMap = new HashMap<>();

                        IgniteCacheSnapshotManager snp = cctx.snapshot();

                        for (final CacheGroupContext grp : cctx.cache().cacheGroups()) {
                            long delay = grp.config().getRebalanceDelay();

                            boolean disableRebalance = snp.partitionsAreFrozen(grp);

                            GridDhtPreloaderAssignments assigns = null;

                            // Don't delay for dummy reassigns to avoid infinite recursion.
                            if ((delay == 0 || forcePreload) && !disableRebalance)
                                assigns = grp.preloader().generateAssignments(exchId, exchFut);

                            assignsMap.put(grp.groupId(), assigns);
                        }
                    }
                }
                finally {
                    // Must flip busy flag before assignments are given to demand workers.
                    busy = false;
                }

                if (assignsMap != null) {
                    int size = assignsMap.size();

                    NavigableMap<Integer, List<Integer>> orderMap = new TreeMap<>();

                    for (Map.Entry<Integer, GridDhtPreloaderAssignments> e : assignsMap.entrySet()) {
                        int grpId = e.getKey();

                        CacheGroupContext grp = cctx.cache().cacheGroup(grpId);

                        int order = grp.config().getRebalanceOrder();

                        if (orderMap.get(order) == null)
                            orderMap.put(order, new ArrayList<Integer>(size));

                        orderMap.get(order).add(grpId);
                    }

                    Runnable r = null;

                    List<String> rebList = new LinkedList<>();

                    boolean assignsCancelled = false;

                    GridCompoundFuture<Boolean, Boolean> forcedRebFut = null;

                    if (task instanceof ForceRebalanceExchangeTask)
                        forcedRebFut = ((ForceRebalanceExchangeTask)task).forcedRebalanceFuture();

                    for (Integer order : orderMap.descendingKeySet()) {
                        for (Integer grpId : orderMap.get(order)) {
                            CacheGroupContext grp = cctx.cache().cacheGroup(grpId);

                            GridDhtPreloaderAssignments assigns = assignsMap.get(grpId);

                            if (assigns != null)
                                assignsCancelled |= assigns.cancelled();

                            // Cancels previous rebalance future (in case it's not done yet).
                            // Sends previous rebalance stopped event (if necessary).
                            // Creates new rebalance future.
                            // Sends current rebalance started event (if necessary).
                            // Finishes cache sync future (on empty assignments).
                            Runnable cur = grp.preloader().addAssignments(assigns,
                                forcePreload,
                                cnt,
                                r,
                                forcedRebFut);

                            if (cur != null) {
                                rebList.add(grp.cacheOrGroupName());

                                r = cur;
                            }
                        }
                    }

                    if (forcedRebFut != null)
                        forcedRebFut.markInitialized();

                    if (assignsCancelled) { // Pending exchange.
                        U.log(log, "Skipping rebalancing (obsolete exchange ID) " +
                            "[top=" + resVer + ", evt=" + exchId.discoveryEventName() +
                            ", node=" + exchId.nodeId() + ']');
                    }
                    else if (r != null) {
                        Collections.reverse(rebList);

                        U.log(log, "Rebalancing scheduled [order=" + rebList + "]");

                        if (!hasPendingExchange()) {
                            U.log(log, "Rebalancing started " +
                                "[top=" + resVer + ", evt=" + exchId.discoveryEventName() +
                                ", node=" + exchId.nodeId() + ']');

                            r.run(); // Starts rebalancing routine.
                        }
                        else
                            U.log(log, "Skipping rebalancing (obsolete exchange ID) " +
                                "[top=" + resVer + ", evt=" + exchId.discoveryEventName() +
                                ", node=" + exchId.nodeId() + ']');
                    }
                    else
                        U.log(log, "Skipping rebalancing (nothing scheduled) " +
                            "[top=" + resVer + ", evt=" + exchId.discoveryEventName() +
                            ", node=" + exchId.nodeId() + ']');
                }
            }
            catch (IgniteInterruptedCheckedException e) {
                throw e;
            }
            catch (IgniteClientDisconnectedCheckedException | IgniteNeedReconnectException e) {
                if (cctx.discovery().reconnectSupported()) {
                    U.warn(log, "Local node failed to complete partition map exchange due to " +
                        "exception, will try to reconnect to cluster: " + e.getMessage(), e);

                    cctx.discovery().reconnect();

                    reconnectNeeded = true;
                }
                else
                    U.warn(log, "Local node received IgniteClientDisconnectedCheckedException or " +
                        " IgniteNeedReconnectException exception but doesn't support reconnect, stopping node: " +
                        e.getMessage(), e);

                return;
            }
            catch (IgniteCheckedException e) {
                U.error(log, "Failed to wait for completion of partition map exchange " +
                    "(preloading will not start): " + task, e);

                throw e;
            }
        }
    }
}